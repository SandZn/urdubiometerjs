{"version":3,"sources":["webpack://UrduBioMeter/webpack/bootstrap","webpack://UrduBioMeter/./src/graphparser.js","webpack://UrduBioMeter/./src/index.js"],"names":["installedModules","__webpack_require__","moduleId","exports","module","i","l","modules","call","m","c","d","name","getter","o","Object","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","prototype","hasOwnProperty","p","s","[object Object]","settings","json_settings","alert","this","graph","onmatchRules","map","x","prevClasses","nextClasses","production","onmatchRules_lookup","rules","prevTokens","tokens","nextTokens","cost","tokenizer","RegExp","whitespace","default","tokenClass","consolidate","message","console","log","input","self","matches","_matches","lastIndex","prevWhitespace","isWhitespace","token","includes","exec","push","index","ValueError","length","pop","startIdx","constraintVal","checkPrev","checkNext","byClass","_input_tokens","source","target","tokenIdx","startAt","constraints","edge","constraintType","node","matchTokens","matchAll","stack","appendChildren","nodeKey","childKey","rulesKeys","rulesKey","children","orderedChildren","j","parentKey","currNode","matchConstraints","append","currMatchRules","prevToken","currToken","currTokenRules","tokensMatched","ruleKey","rule","tokenize","output","_ruleKeys","matchAt","onmatchIdx","onmatch","GraphParser"],"mappings":"6BACA,IAAAA,KAGA,SAAAC,EAAAC,GAGA,GAAAF,EAAAE,GACA,OAAAF,EAAAE,GAAAC,QAGA,IAAAC,EAAAJ,EAAAE,IACAG,EAAAH,EACAI,GAAA,EACAH,YAUA,OANAI,EAAAL,GAAAM,KAAAJ,EAAAD,QAAAC,IAAAD,QAAAF,GAGAG,EAAAE,GAAA,EAGAF,EAAAD,QA0DA,OArDAF,EAAAQ,EAAAF,EAGAN,EAAAS,EAAAV,EAGAC,EAAAU,EAAA,SAAAR,EAAAS,EAAAC,GACAZ,EAAAa,EAAAX,EAAAS,IACAG,OAAAC,eAAAb,EAAAS,GAA0CK,YAAA,EAAAC,IAAAL,KAK1CZ,EAAAkB,EAAA,SAAAhB,GACA,oBAAAiB,eAAAC,aACAN,OAAAC,eAAAb,EAAAiB,OAAAC,aAAwDC,MAAA,WAExDP,OAAAC,eAAAb,EAAA,cAAiDmB,OAAA,KAQjDrB,EAAAsB,EAAA,SAAAD,EAAAE,GAEA,GADA,EAAAA,IAAAF,EAAArB,EAAAqB,IACA,EAAAE,EAAA,OAAAF,EACA,KAAAE,GAAA,iBAAAF,QAAAG,WAAA,OAAAH,EACA,IAAAI,EAAAX,OAAAY,OAAA,MAGA,GAFA1B,EAAAkB,EAAAO,GACAX,OAAAC,eAAAU,EAAA,WAAyCT,YAAA,EAAAK,UACzC,EAAAE,GAAA,iBAAAF,EAAA,QAAAM,KAAAN,EAAArB,EAAAU,EAAAe,EAAAE,EAAA,SAAAA,GAAgH,OAAAN,EAAAM,IAAqBC,KAAA,KAAAD,IACrI,OAAAF,GAIAzB,EAAA6B,EAAA,SAAA1B,GACA,IAAAS,EAAAT,KAAAqB,WACA,WAA2B,OAAArB,EAAA,SAC3B,WAAiC,OAAAA,GAEjC,OADAH,EAAAU,EAAAE,EAAA,IAAAA,GACAA,GAIAZ,EAAAa,EAAA,SAAAiB,EAAAC,GAAsD,OAAAjB,OAAAkB,UAAAC,eAAA1B,KAAAuB,EAAAC,IAGtD/B,EAAAkC,EAAA,GAIAlC,IAAAmC,EAAA,uEChFAhC,EAAAD,cAOAkC,YAAAC,GACAC,cAAAtC,EAAA,GACAuC,MAAAD,eACAE,KAAAC,MAAAJ,EAAA,OACAG,KAAAE,aAAAL,EAAA,eAAAM,IAAA,SAAAC,GACA,OACAC,YAAAD,EAAA,GACAE,YAAAF,EAAA,GACAG,WAAAH,EAAA,MAGAJ,KAAAQ,oBAAAX,EAAA,sBACAG,KAAAS,MAAAZ,EAAA,OAAAM,IAAA,SAAAC,GACA,OACAG,WAAAH,EAAA,GACAC,YAAAD,EAAA,GACAM,WAAAN,EAAA,GACAO,OAAAP,EAAA,GACAQ,WAAAR,EAAA,GACAE,YAAAF,EAAA,GACAS,KAAAT,EAAA,MAGAJ,KAAAc,UAAA,IAAAC,OAAAlB,EAAA,wBACAG,KAAAW,OAAAd,EAAA,QACA,IAAAO,EAAAP,EAAA,YACAG,KAAAgB,YACAC,QAAAb,EAAA,GACAc,WAAAd,EAAA,GACAe,YAAAf,EAAA,IAQAR,WAAAwB,GACAC,QAAAC,IAAAF,GAOAxB,SAAA2B,GAWA,IAVA,IASAvD,EATAwD,EAAAxB,KACAW,KACAc,GAAAD,EAAAR,WAAAC,SACAS,KACAC,EAAA,EACAC,GAAA,EACAC,EAAA,SAAAC,GACA,OAAAN,EAAAb,OAAAmB,GAAAC,SAAAP,EAAAR,WAAAE,aAGA,QAAAlD,EAAAwD,EAAAV,UAAAkB,KAAAT,KACAG,EAAAO,KAAAjE,GACAyD,EAAAQ,KAAAjE,EAAA,IACAA,EAAAkE,QAAAP,GACAH,EAAAW,WAAA,6BAAAR,EAAA,OAAAJ,GAEAI,EAAAD,IAAAU,OAAA,GAAAF,MACAR,IAAAU,OAAA,MAAAA,OAEA,OAAAX,EAAAW,OAEA,OADAZ,EAAAW,WAAA,kCAAAZ,MAGA,QAAA3D,EAAA,EAAmBA,EAAA6D,EAAAW,OAAoBxE,IAAA,CACvC,IAAAkE,EAAAL,EAAA7D,GACA,GAAAiE,EAAAC,GAAA,CACA,GAAAF,GAAAJ,EAAAR,WAAAG,YACA,SAEAS,GAAA,OAGAA,GAAA,EAEAjB,EAAAsB,KAAAH,GAEA,GAAAN,EAAAR,WAAAG,YACA,KAAAU,EAAAlB,IAAAyB,OAAA,KACAzB,EAAA0B,MAUA,OAPA1B,EAAAsB,KAAAT,EAAAR,WAAAC,SACAU,IAAAJ,EAAAa,QACAZ,EAAAW,WACA,+BAAAR,EACA,OAAAJ,GAGAZ,EAWAf,YAAA0C,EAAAC,EAAAC,EAAAC,EAAAC,GACA,IAAA/B,EAAAX,KAAA2C,cACA,GAAAH,GAAAF,EAAA,EACA,SAEA,GAAAG,GAAAH,EAAAC,EAAAH,OAAAzB,EAAAyB,OACA,SAEA,QAAAxE,EAAA,EAAmBA,EAAA2E,EAAAH,OAA0BxE,IAC7C,GAAA8E,GACA,IAAA1C,KAAAW,SAAA2B,EAAA1E,IAAAmE,SAAAQ,EAAA3E,IACA,cAEO,GAAA+C,EAAA2B,EAAA1E,KAAA2E,EAAA3E,GACP,SAGA,SASAgC,iBAAAgD,EAAAC,EAAAC,GACA,IAQAP,EAAA7B,EAAAE,EAAAmC,EANAC,EAFAhD,KACAC,MAAAgD,KAAAL,GAAAC,GACA,YAEA,IAAAG,EACA,SAKA,QAAAE,KAAAF,EAEA,GADAT,EAAAS,EAAAE,GACA,gBAAAA,GAKA,GAHAH,EAAAD,EACAC,GAfA/C,KAaAS,MAbAT,KAaAC,MAAAkD,KAAAN,GAAA,UAAAlC,OAAAyB,OAGAW,GAAAR,EAAAH,QAhBApC,KAiBAoD,YAAAL,EAAAR,GAAA,SACA,cAEO,mBAAAW,GAEP,GADAH,EAAAD,GArBA9C,KAsBAoD,YAAAL,EAAAR,GAAA,SACA,cAEO,oBAAAW,GASP,GAPAH,EAAAD,EACAC,GA5BA/C,KA0BAS,MA1BAT,KA0BAC,MAAAkD,KAAAN,GAAA,UAAAlC,OAAAyB,QAGA1B,EAAAsC,EAAA,eAEAD,GAAArC,EAAA0B,QAEAW,GAAAR,EAAAH,QACApC,KAAAoD,YAAAL,EAAAR,GAAA,SACA,cAEO,oBAAAW,IACPH,EAAAD,GACAlC,EAAAoC,EAAA,eAEAD,GAAAnC,EAAAwB,SAEApC,KAAAoD,YAAAL,EAAAR,GAAA,UACA,SAIA,SAQA3C,QAAAkD,EAAAO,GACA,GAAAA,EACA,IAAA5B,KAEA,IACAd,EADAX,KACA2C,cACA1C,EAFAD,KAEAC,MACAqD,KACAC,EAAA,SAAAC,EAAAV,GACA,IAEAW,EAAAC,EAAAC,EAFAC,EAAA,KACAC,EAAA5D,EAAAkD,KAAAK,GAAA,iBAGA,GAAAK,EAEA,GADAD,EAAAC,EAAAlD,EAAAmC,IAEA,QAAAlF,EAAAgG,EAAAxB,OAAuCxE,GAAA,EAAQA,IAC/C6F,EAAAG,EAAAhG,GAEA0F,EAAArB,MAAAwB,EAAAD,EAAAV,SAIA,GADAY,EAAAG,EAAA,UAEA,QAAAC,EAAAJ,EAAAtB,OAA0C0B,GAAA,EAAQA,IAClDH,EAAAD,EAAAI,GACAR,EAAArB,MAAA0B,EAAAH,EAAAV,KASA,IAFAS,EAAA,EAAAT,GAEAQ,EAAAlB,OAAA,IACA,IAAAhC,EAAAkD,EAAAjB,MACAmB,EAAApD,EAAA,GACA2D,EAAA3D,EAAA,GACA0C,EAAA1C,EAAA,GACA,IAAA4D,EAAA/D,EAAAkD,KAAAK,GAEA,GAAAQ,EAAA,WAtCAhE,KAuCAiE,iBAAAF,EAAAP,EAAAV,GADA,CAEA,IAAAO,EAGA,OAAAW,EAAA,SAFAvC,EAAAyC,OAAAF,EAAA,eAMAlB,EAAAnC,EAAAyB,OAAA,IACAU,GAAA,GAEAS,EAAAC,EAAAV,GAEK,GAAAO,EACL,OAAA5B,EArDAzB,KAuDAmC,WACA,yBAAAxB,EAAAmC,GACA,WAAAA,EACA,OAAAnC,GASAf,MAAA2B,GACA,IAIA4C,EAAAC,EAAAC,EAAAC,EAAAC,EACAC,EAAAC,EAJA9D,EADAX,KACA0E,SAAAnD,GACAoD,EAAA,GACA7B,EAAA,EAKA,IARA9C,KAMA2C,cAAAhC,EANAX,KAOA4E,aACA9B,EAAAnC,EAAAyB,OAAA,IAKA,GAJAoC,EATAxE,KASA6E,QAAA/B,GAEAyB,GADAE,EAVAzE,KAUAS,MAAA+D,IACA7D,OAXAX,KAYA4E,UAAA3C,KAAAuC,GAZAxE,KAaAE,eACAiE,EAAA,KACAC,EAAAzD,EAAAmC,EAAA,GACAuB,EAAA1D,EAAAmC,IACAwB,EAjBAtE,KAiBAQ,oBAAA6D,MAEAF,EAAAG,EAAAF,IAEAD,GACA,QAAAL,EAAA,EAAyBA,EAAAK,EAAA/B,OAA2B0B,IAAA,CACpD,IAAAgB,EAAAX,EAAAL,GACAiB,EAxBA/E,KAwBAE,aAAA4E,GACA,GAzBA9E,KAyBAoD,YACAN,EAAAiC,EAAA1E,YAAA+B,OACA2C,EAAA1E,aACA,UA5BAL,KA6BAoD,YACAN,EACAiC,EAAAzE,aACA,SACA,CACAqE,GAAAI,EAAAxE,WACA,OAKAoE,GAAAF,EAAAlE,WACAuC,GAAAyB,EAAAnC,OAEA,OAAAuC,kCCtTAhH,EAAAD,SACAsH,YAAAxH,EAAA","file":"urdubiometer.js","sourcesContent":[" \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 2);\n","'use strict'\n\nmodule.exports = class GraphParser {\n  /**\n    * Graph-based parser.\n    * @constructor\n    * @param {object} settings UrduBioMeter graph parser serialization\n    * @return {object}\n    */\n  constructor (settings) {\n    json_settings = require('../src/settings.json')\n    alert(json_settings)\n    this.graph = settings['_graph']\n    this.onmatchRules = settings['_onmatch_rules'].map(function (x) {\n      return {\n        prevClasses: x[0],\n        nextClasses: x[1],\n        production: x[2]\n      }\n    })\n    this.onmatchRules_lookup = settings['_onmatch_rules_lookup']\n    this.rules = settings['_rules'].map(function (x) {\n      return {\n        production: x[0],\n        prevClasses: x[1],\n        prevTokens: x[2],\n        tokens: x[3],\n        nextTokens: x[4],\n        nextClasses: x[5],\n        cost: x[6]\n      }\n    })\n    this.tokenizer = new RegExp(settings['_tokenizer_pattern'], 'g')\n    this.tokens = settings['_tokens']\n    var x = settings['_whitespace']\n    this.whitespace = {\n      default: x[0],\n      tokenClass: x[1],\n      consolidate: x[2]\n    }\n  };\n\n  /**\n   * Handle value error during parse.\n   * @param {string} message Error message.\n   */\n  valueError (message) {\n    console.log(message)\n  }\n\n  /**\n   * @param {string} input string to tokenize\n   * @return {string[]} list of tokens\n   */\n  tokenize (input) {\n    var self = this\n    var tokens = []\n    var matches = [self.whitespace.default]\n    var _matches = []\n    var lastIndex = 0\n    var prevWhitespace = false\n    var isWhitespace = function (token) {\n      return self.tokens[token].includes(self.whitespace.tokenClass)\n    }\n    var m\n    while ((m = self.tokenizer.exec(input)) !== null) {\n      _matches.push(m)\n      matches.push(m[1])\n      if (m.index !== lastIndex) {\n        self.ValueError('Unrecognized token at pos ' + lastIndex + ' of ' + input)\n      }\n      lastIndex = _matches[_matches.length - 1].index +\n                   _matches[_matches.length - 1][1].length\n    }\n    if (matches.length === 1) {\n      self.ValueError('Unrecognized token at pos 0 of ' + input)\n      return []\n    }\n    for (var i = 0; i < matches.length; i++) {\n      var token = matches[i]\n      if (isWhitespace(token)) {\n        if (prevWhitespace && self.whitespace.consolidate) {\n          continue\n        } else {\n          prevWhitespace = true\n        }\n      } else {\n        prevWhitespace = false\n      }\n      tokens.push(token)\n    }\n    if (self.whitespace.consolidate) {\n      while (isWhitespace(tokens[tokens.length - 1])) {\n        tokens.pop()\n      }\n    }\n    tokens.push(self.whitespace.default)\n    if (lastIndex !== input.length) {\n      self.ValueError(\n        'Unrecognizable input at pos ' + lastIndex +\n          ' of ' + input\n      )\n    }\n    return tokens\n  }\n\n  /**\n   * @param {number} startIdx index of token to start match at\n   * @param {[]} constraintVal\n   * @param {boolean} checkPrev\n   * @param {boolean} checkNext\n   * @param {boolean} byClass\n   * @return {boolean}\n   */\n  matchTokens (startIdx, constraintVal, checkPrev, checkNext, byClass) {\n    var tokens = this._input_tokens\n    if (checkPrev && startIdx < 0) {\n      return false\n    }\n    if (checkNext && startIdx + constraintVal.length > tokens.length) {\n      return false\n    }\n    for (var i = 0; i < constraintVal.length; i++) {\n      if (byClass) {\n        if (!this.tokens[tokens[startIdx + i]].includes(constraintVal[i])) {\n          return false\n        }\n      } else if (tokens[startIdx + i] !== constraintVal[i]) {\n        return false\n      }\n    }\n    return true\n  }\n\n  /**\n   * @param source\n   * @param target\n   * @param {number} tokenIdx\n   * @return {boolean}\n   */\n  matchConstraints (source, target, tokenIdx) {\n    var self = this\n    var targetEdge = self.graph.edge[source][target]\n    var constraints = targetEdge['constraints']\n\n    if (!constraints) {\n      return true\n    }\n\n    var constraintVal, prevTokens, nextTokens, numTokens, startAt\n\n    for (var constraintType in constraints) {\n      constraintVal = constraints[constraintType]\n      if (constraintType === 'prev_tokens') {\n        numTokens = self.rules[self.graph.node[target]['rule_key']].tokens.length\n        startAt = tokenIdx\n        startAt -= numTokens\n        startAt -= constraintVal.length\n        if (!self.matchTokens(startAt, constraintVal, true, false, false)) {\n          return false\n        }\n      } else if (constraintType === 'next_tokens') {\n        startAt = tokenIdx\n        if (!self.matchTokens(startAt, constraintVal, false, true, false)) {\n          return false\n        }\n      } else if (constraintType === 'prev_classes') {\n        numTokens = self.rules[self.graph.node[target]['rule_key']].tokens.length\n        startAt = tokenIdx\n        startAt -= numTokens\n        prevTokens = constraints['prev_tokens']\n        if (prevTokens) {\n          startAt -= prevTokens.length\n        }\n        startAt -= constraintVal.length\n        if (!this.matchTokens(startAt, constraintVal, true, false, true)) {\n          return false\n        }\n      } else if (constraintType === 'next_classes') {\n        startAt = tokenIdx\n        nextTokens = constraints['next_tokens']\n        if (nextTokens) {\n          startAt += nextTokens.length\n        }\n        if (!this.matchTokens(startAt, constraintVal, false, true, true)) {\n          return false\n        }\n      }\n    }\n    return true\n  }\n\n  /**\n  * @param {number} tokenIdx\n  * @param {boolean} matchAll\n  * @return {number|number[]}\n  */\n  matchAt (tokenIdx, matchAll) {\n    if (matchAll) {\n      var matches = []\n    }\n    var self = this\n    var tokens = self._input_tokens\n    var graph = self.graph\n    var stack = []\n    var appendChildren = function (nodeKey, tokenIdx) {\n      var children = null\n      var orderedChildren = graph.node[nodeKey]['ordered_children']\n      var childKey, rulesKeys, rulesKey\n\n      if (orderedChildren) {\n        children = orderedChildren[tokens[tokenIdx]]\n        if (children) {\n          for (var i = children.length; i >= 0; i--) {\n            childKey = children[i]\n            /* stack (LIFO) from right side */\n            stack.push([childKey, nodeKey, tokenIdx])\n          }\n        } else {\n          rulesKeys = orderedChildren['__rules__']\n          if (rulesKeys) {\n            for (var j = rulesKeys.length; j >= 0; j--) {\n              rulesKey = rulesKeys[j]\n              stack.push([rulesKey, nodeKey, tokenIdx])\n            }\n          }\n        }\n      }\n    }\n\n    appendChildren(0, tokenIdx)\n\n    while (stack.length > 0) {\n      var x = stack.pop()\n      var nodeKey = x[0]\n      var parentKey = x[1]\n      tokenIdx = x[2]\n      var currNode = graph.node[nodeKey]\n\n      if (currNode['accepting'] &&\n        self.matchConstraints(parentKey, nodeKey, tokenIdx)) {\n        if (matchAll) {\n          matches.append(currNode['rule_key'])\n        } else {\n          return currNode['rule_key']\n        }\n        continue\n      } else {\n        if (tokenIdx < tokens.length - 1) {\n          tokenIdx += 1\n        }\n        appendChildren(nodeKey, tokenIdx)\n      }\n    } if (matchAll) {\n      return matches\n    } else {\n      self.ValueError(\n        'Could not match token ' + tokens[tokenIdx] +\n        ' at pos ' + tokenIdx +\n        ' in ' + tokens\n      )\n    }\n  }\n\n  /**\n  * @param {string} input\n  * @return {string}\n  */\n  parse (input) {\n    var self = this\n    var tokens = self.tokenize(input)\n    var output = ''\n    var tokenIdx = 1\n    var currMatchRules, prevToken, currToken, currTokenRules, tokensMatched,\n      ruleKey, rule\n    self._input_tokens = tokens\n    self._ruleKeys = []\n    while (tokenIdx < tokens.length - 1) {\n      ruleKey = self.matchAt(tokenIdx)\n      rule = self.rules[ruleKey]\n      tokensMatched = rule.tokens\n      self._ruleKeys.push(ruleKey)\n      if (self.onmatchRules) {\n        currMatchRules = null\n        prevToken = tokens[tokenIdx - 1]\n        currToken = tokens[tokenIdx]\n        currTokenRules = self.onmatchRules_lookup[currToken]\n        if (currTokenRules) {\n          currMatchRules = currTokenRules[prevToken]\n        }\n        if (currMatchRules) {\n          for (var j = 0; j < currMatchRules.length; j++) {\n            var onmatchIdx = currMatchRules[j]\n            var onmatch = self.onmatchRules[onmatchIdx]\n            if (self.matchTokens(\n              tokenIdx - onmatch.prevClasses.length,\n              onmatch.prevClasses,\n              true, false, true\n            ) && self.matchTokens(\n                tokenIdx,\n                onmatch.nextClasses,\n                false, true, true\n              )) {\n              output += onmatch.production\n              break\n            }\n          }\n        }\n      }\n      output += rule.production\n      tokenIdx += tokensMatched.length\n    }\n    return output\n  }\n}\n","'use strict'\n\nmodule.exports = {\n  GraphParser: require('./graphparser')\n}\n"],"sourceRoot":""}